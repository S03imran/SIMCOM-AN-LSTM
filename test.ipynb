{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('sklearn: {}'. format(sklearn. __version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict_main = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_path(adj, parameter=0.05):\n",
    "    G = nx.Graph(adj)\n",
    "    adj2 = nx.to_numpy_matrix(G)\n",
    "    common = np.zeros(shape=(len(adj), len(adj)))\n",
    "    common1 = np.zeros(shape=(len(adj), len(adj)))\n",
    "    common2 = np.zeros(shape=(len(adj), len(adj)))\n",
    "    adj2 = np.zeros(shape=(len(adj), len(adj)))\n",
    "    for (u, v) in G.edges:\n",
    "        adj2[u][v] = 1\n",
    "        adj2[v][u] = 1\n",
    "    print(len(G.edges))\n",
    "    print(\"before lp matmul1\")\n",
    "    common1 = np.matmul(adj2, adj2)\n",
    "    print(\"before lp matmul2\")\n",
    "    common2 = np.matmul(common1, adj2)\n",
    "    common2 = common2 * parameter\n",
    "    for i in range(len(adj)):\n",
    "        for j in range(len(adj)):\n",
    "            common[i][j] = common1[i][j] + common2[i][j]\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_edges_pool_single(num):\n",
    "    starttime = time.time()\n",
    "    G = var_dict_main['graph']\n",
    "    print(\"inside pool num = \"+str(num))\n",
    "    seen = set()\n",
    "    x, y = random.choice(list(G.nodes)), random.choice(list(G.nodes))\n",
    "    t = 0\n",
    "    while t < num:\n",
    "        if not G.has_edge(x, y) and (y, x) not in seen:\n",
    "            seen.add((x, y))\n",
    "            t = t + 1\n",
    "            #print(t)\n",
    "        x, y = random.choice(list(G.nodes)), random.choice(list(G.nodes))\n",
    "\n",
    "    print(\"after random edge generation inside pool \"+str(len(seen)))\n",
    "    endtime = time.time()\n",
    "    currentDT = datetime.datetime.now()\n",
    "    print(str(currentDT))\n",
    "    #print(len(seen_inside))\n",
    "    #print(seen_inside)\n",
    "\n",
    "    return seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_edges(num, G):\n",
    "    #print(str(G.nodes))\n",
    "    #print(str(G.edges))\n",
    "    num = num*5\n",
    "    print(\"generating random edges\")\n",
    "    print(type(G))\n",
    "    print(\"nodes = \"+str(len(G.nodes)))\n",
    "    print(\"edges = \"+str(len(G.edges)))\n",
    "    cpu = mp.cpu_count()\n",
    "    cpu = cpu - 1\n",
    "    print(\"cpu cores = \" + str(cpu))\n",
    "    arglist = []\n",
    "    for i in range(cpu):\n",
    "        arglist.append(int(num/cpu))\n",
    "    var_dict_main['graph'] = G\n",
    "    seen = set()\n",
    "    with mp.Pool(cpu) as p:\n",
    "        element = p.map(gen_rand_edges_pool_single,arglist)\n",
    "        for e in element:\n",
    "            seen.update(e)\n",
    "    x, y = random.choice(list(G.nodes)), random.choice(list(G.nodes))\n",
    "    t = len(seen)\n",
    "    print(\"outside pool t = \"+str(t)+\" num = \"+str(num))\n",
    "    #print(seen)\n",
    "    #time.sleep(100)\n",
    "    while t < num:\n",
    "        if not G.has_edge(x,y) and (y, x) not in seen:\n",
    "            seen.add((x, y))\n",
    "            t = t + 1\n",
    "        x, y = random.choice(list(G.nodes)), random.choice(list(G.nodes))\n",
    "    seen = list(seen)\n",
    "    print(\"after random edge generation\")\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(m, t):\n",
    "    print(\"reading dataset from file\")\n",
    "    data = open('./datasets_dynamic/'+str(t) + \".txt\")\n",
    "    edgelist = map(lambda q: list(map(int, q.split())), data.read().split(\"\\n\")[:-1])\n",
    "    data.close()\n",
    "    maxi = 0\n",
    "    mini = 100000000000000000\n",
    "    edgelist = list(edgelist)\n",
    "    for x in edgelist:\n",
    "        if x[-1] > maxi:\n",
    "            maxi = x[-1]\n",
    "        if x[-1] < mini:\n",
    "            mini = x[-1]\n",
    "    min1 = mini\n",
    "    w = int((maxi - mini) / m)\n",
    "    edgelist.sort(key=lambda x: x[-1])\n",
    "    arr = []\n",
    "    i = 0\n",
    "    for i in range(0, m + 1):\n",
    "        arr = arr + [min1 + w * i]\n",
    "    arri = []\n",
    "    # print(arr)\n",
    "    nodes = set()\n",
    "    for i in range(0, m):\n",
    "        temp = []\n",
    "        for j in edgelist:\n",
    "            if j[-1] >= arr[i] and j[-1] <= arr[i + 1] and j[0]!=j[1]:\n",
    "                temp += [[j[0], j[1]]]\n",
    "        if temp != []:\n",
    "            arri += [temp]\n",
    "    # print(arri)\n",
    "    # for x in arri:\n",
    "    #     print(len(x))\n",
    "    print(\"after read\")\n",
    "    #random.shuffle(arri)\n",
    "    #print(str(arri))\n",
    "    for item in arri: print(len(item))\n",
    "    return arri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(l):\n",
    "    print(\"inside gen graph\")\n",
    "    t_graph = []\n",
    "    node_set = set()\n",
    "    max = -99999\n",
    "    min = 99999\n",
    "    for i in l:\n",
    "        for edge in i:\n",
    "            node_set.add(edge[0])\n",
    "            node_set.add(edge[1])\n",
    "            u = edge[0]\n",
    "            v = edge[1]\n",
    "            if u<v:\n",
    "                if min > u:\n",
    "                   min = u\n",
    "                if max < v:\n",
    "                    max = v\n",
    "            else:\n",
    "                if min > v:\n",
    "                   min = v\n",
    "                if max < u:\n",
    "                    max = u\n",
    "    print(str(min) + \"-\" + str(max))\n",
    "    #sys.exit()\n",
    "    edgelist_new = []\n",
    "    count = -1\n",
    "    for i in l:\n",
    "        graph = nx.Graph()\n",
    "        #graph.add_nodes_from(node_set)\n",
    "        graph.add_edges_from(i)\n",
    "        graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "        t_graph.append(graph)\n",
    "        edgelist_new.append(list(graph.edges))\n",
    "    return [t_graph,edgelist_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_to_lp (x,y, graph, comm_list):\n",
    "    count = 0\n",
    "    comm_dict = dict()\n",
    "    comm_no_indi = dict()\n",
    "    #create dictionary which gives node communities\n",
    "    for item in comm_list:\n",
    "        count += 1\n",
    "        comm_no_indi[count] = len(item)\n",
    "        for node in item:\n",
    "            comm_dict[node] = count\n",
    "    #case to check faulty algorithm\n",
    "    for node in graph:\n",
    "        if node not in comm_dict.keys():\n",
    "            print(\"non labelled node found\")\n",
    "            sys.exit()\n",
    "    #getting 3 hop regions of x and y nodes\n",
    "    reg_x = set()\n",
    "    reg_y = set()\n",
    "    reg_x.add(x)\n",
    "    reg_y.add(y)\n",
    "    #for 3 hop region\n",
    "    for i in range(3):\n",
    "        temp_x = set()\n",
    "        temp_y = set()\n",
    "        for curr in reg_x:\n",
    "            temp_x.add(curr)\n",
    "            for single in graph.neighbors(curr):\n",
    "                temp_x.add(single)\n",
    "        for curr in reg_y:\n",
    "            temp_y.add(curr)\n",
    "            for single in graph.neighbors(curr):\n",
    "                temp_y.add(single)\n",
    "        reg_x = temp_x\n",
    "        reg_y = temp_y\n",
    "    reg_x.remove(x)\n",
    "    reg_y.remove(y)\n",
    "    #intersection of ego regions\n",
    "    common_reg = reg_x.intersection(reg_y)\n",
    "    lp = 0\n",
    "    count_comm = dict()\n",
    "    #count number of instances of each community in region of common infleunce\n",
    "    for key in comm_no_indi.keys():\n",
    "        count_comm[key] = 0\n",
    "    for item in common_reg:\n",
    "        count_comm[comm_dict[item]] += 1\n",
    "    for key in comm_no_indi.keys():\n",
    "        if comm_dict[x] == comm_dict[y] == key:\n",
    "            # special case for 3 equal labels to be defined\n",
    "            #count_comm[key] = count_comm[key] / comm_no_indi[key]\n",
    "            count_comm[key] = count_comm[key]\n",
    "        else:\n",
    "            count_comm[key] = count_comm[key]/comm_no_indi[key]\n",
    "    for key in count_comm.keys():\n",
    "        lp += count_comm[key]\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_comm_opti(m, t, f_no=12):\n",
    "    identity = \" algo - comm dyn dataset - \" + str(t)\n",
    "\n",
    "    data_curr = data(m,t)\n",
    "    l1 = data_curr\n",
    "    l = []\n",
    "    for i in l1:\n",
    "        edgelist = list(set(tuple(sorted(sub)) for sub in i))\n",
    "        l.append(edgelist)\n",
    "\n",
    "    t_graph = gen_graph(l)[0]\n",
    "    l = gen_graph(l)[1]\n",
    "\n",
    "    teCt = l[m - 1]\n",
    "    reCt = gen_rand_edges(len(teCt), t_graph[m - 1])\n",
    "\n",
    "    for i in reCt:\n",
    "        teCt.append(i)\n",
    "\n",
    "    teCt = list(set(tuple(sorted(sub)) for sub in teCt))\n",
    "\n",
    "    list1 = []\n",
    "    print(\"before graph\" + str(identity))\n",
    "    edges_dict = dict()\n",
    "    for t1 in range(0, m - 1):\n",
    "        starttime = time.time()\n",
    "        print(\"before dictionary\" + str(identity))\n",
    "        sum = 0\n",
    "        count = 0\n",
    "        dict_count = 0\n",
    "        dict_node = dict()\n",
    "        G = t_graph[t1]\n",
    "        print(str(G))\n",
    "        for node in G.nodes:\n",
    "            dict_node[node] = dict_count\n",
    "            dict_count += 1\n",
    "        currentDT = datetime.datetime.now()\n",
    "        print(str(currentDT))\n",
    "        from cdlib.algorithms import cpm, label_propagation, chinesewhispers,\\\n",
    "            der, eigenvector, sbm_dl_nested, walktrap, surprise_communities, \\\n",
    "            spinglass, significance_communities, rb_pots, rber_pots, pycombo, \\\n",
    "            paris, mcode, markov_clustering, lswl_plus, infomap, greedy_modularity, \\\n",
    "            leiden\n",
    "        \n",
    "        current_comm = greedy_modularity(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_greed = current_comm_dict['communities']\n",
    "        print(\"no. of communities greed - \" + str(len(current_comm_list_greed)))\n",
    "        for item in current_comm_list_greed:\n",
    "            print(item)\n",
    "        print(\"after community greed\")\n",
    "        \n",
    "        current_comm = eigenvector(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_eigen = current_comm_dict['communities']\n",
    "        print(\"no. of communities eigen - \" + str(len(current_comm_list_eigen)))\n",
    "        for item in current_comm_list_eigen:\n",
    "            print(item)\n",
    "        print(\"after community eigen\")\n",
    "        \n",
    "        current_comm = cpm(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_cpm = current_comm_dict['communities']\n",
    "        print(\"no. of communities cpm - \" + str(len(current_comm_list_cpm)))\n",
    "        for item in current_comm_list_cpm:\n",
    "            print(item)\n",
    "        print(\"after community cpm\")\n",
    "        \n",
    "        current_comm = significance_communities(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_signi = current_comm_dict['communities']\n",
    "        print(\"no. of communities signi - \" + str(len(current_comm_list_signi)))\n",
    "        for item in current_comm_list_signi:\n",
    "            print(item)\n",
    "        print(\"after community signi\")\n",
    "        \n",
    "        current_comm = der(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_der = current_comm_dict['communities']\n",
    "        print(\"no. of communities der - \" + str(len(current_comm_list_der)))\n",
    "        for item in current_comm_list_der:\n",
    "            print(item)\n",
    "        print(\"after community der\")\n",
    "        \n",
    "        current_comm = surprise_communities(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_surp = current_comm_dict['communities']\n",
    "        print(\"no. of communities surp - \" + str(len(current_comm_list_surp)))\n",
    "        for item in current_comm_list_surp:\n",
    "            print(item)\n",
    "        print(\"after community surp\")\n",
    "        \n",
    "        current_comm = sbm_dl_nested(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_sbm = current_comm_dict['communities']\n",
    "        print(\"no. of communities sbm - \" + str(len(current_comm_list_sbm)))\n",
    "        for item in current_comm_list_sbm:\n",
    "            print(item)\n",
    "        print(\"after community sbm\")\n",
    "        \n",
    "        current_comm = leiden(G)\n",
    "        print(type(current_comm))\n",
    "        current_comm_dict = json.loads(current_comm.to_json())\n",
    "        current_comm_list_leiden = current_comm_dict['communities']\n",
    "        print(\"no. of communities leiden - \" + str(len(current_comm_list_leiden)))\n",
    "        for item in current_comm_list_leiden:\n",
    "            print(item)\n",
    "        print(\"after community leiden\")\n",
    "        #sys.exit()\n",
    "        print(len(G.edges))\n",
    "        adj = nx.to_numpy_matrix(G)\n",
    "        edge_length = len(G.edges)\n",
    "        triangles_dict = dict()\n",
    "        \n",
    "        print(\"before laplacian\" + str(identity))\n",
    "        from scipy.sparse.csgraph import laplacian\n",
    "        L = laplacian(adj)\n",
    "        #print(L)\n",
    "        \n",
    "        try:\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print(str(currentDT))\n",
    "            print(\"before p inverse\" + str(identity))\n",
    "            L_pinverse = np.linalg.pinv(L)\n",
    "            np.save(path_pinv, L_pinverse)\n",
    "    #print(L_pinverse)\n",
    "        except:\n",
    "            currentDT = datetime.datetime.now()\n",
    "            print(str(currentDT))\n",
    "            print(\"before empty error p inverse\" + str(identity))\n",
    "            L_pinverse = np.zeros(shape=(len(adj), len(adj)))\n",
    "        \n",
    "        print(\"before inverse\" + str(identity))\n",
    "        I = np.identity(len(adj))\n",
    "        #print(I)\n",
    "        middle = I + L\n",
    "        inverse = np.linalg.inv(middle)\n",
    "        \n",
    "        print(\"before quasi\")\n",
    "        print(\"before local path\" + str(identity))\n",
    "        common_lp = local_path(adj)\n",
    "\n",
    "        print(\"before edges after graph\" + str(identity))\n",
    "        for e in teCt:\n",
    "            i = e[0]\n",
    "            j = e[1]\n",
    "            edge_key = str(e[0]) + \"+\" + str(e[1])\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(str(count) + \" - \" + str(len(teCt)) + \" t1 = \" + str(t1) + str(identity))\n",
    "            # print(e)\n",
    "            list2 = []\n",
    "            curr_tuple = np.zeros(7)\n",
    "            if G.has_node(i) and G.has_node(j):\n",
    "                i_orig = i\n",
    "                j_orig = j\n",
    "                i = dict_node[i]\n",
    "                j = dict_node[j]\n",
    "                ##aa\n",
    "                aa = 0\n",
    "                common_neighbours_all = nx.common_neighbors(G, i_orig, j_orig)\n",
    "                for common_neighbour in common_neighbours_all:\n",
    "                    if G.degree[common_neighbour] != 0 and G.degree[common_neighbour] != 1:\n",
    "                        aa = aa + 1 / math.log(G.degree[common_neighbour])\n",
    "                sum += aa\n",
    "                list2.append(aa)\n",
    "                ##cn\n",
    "                cn = len(sorted(nx.common_neighbors(G, i_orig, j_orig)))\n",
    "                sum += cn\n",
    "                list2.append(cn)\n",
    "                ##pa\n",
    "                pa = 0\n",
    "                n1 = G.neighbors(i_orig)\n",
    "                n2 = G.neighbors(j_orig)\n",
    "                n1 = [item for item in n1]\n",
    "                n2 = [item for item in n2]\n",
    "                length = len(set().union(n1, n2))\n",
    "                if length > 0:\n",
    "                    pa = len(n1) * len(n2)\n",
    "                sum += pa\n",
    "                list2.append(pa)\n",
    "                # cosp\n",
    "                if L_pinverse[j][j] * L_pinverse[i][i] >= 0:\n",
    "                    cosp = L_pinverse[i][j] / (math.sqrt(L_pinverse[j][j] * L_pinverse[i][i]))\n",
    "                else:\n",
    "                    cosp = -1\n",
    "                sum += cosp\n",
    "                if cosp >= 0:\n",
    "                    list2.append(cosp)\n",
    "                else:\n",
    "                    list2.append(0)\n",
    "                # mfi\n",
    "                mfi = inverse[i][j]\n",
    "                sum += mfi\n",
    "                if mfi >= 0:\n",
    "                    list2.append(mfi)\n",
    "                else:\n",
    "                    list2.append(0)\n",
    "                # shortest path\n",
    "                sp = 0\n",
    "                try:\n",
    "                    sp = nx.shortest_path_length(G, i_orig, j_orig)\n",
    "                except:\n",
    "                    sp = 0\n",
    "                    file_write_name = './result_commlp/' + str(t) + '_sp/error.txt'\n",
    "                    os.makedirs(os.path.dirname(file_write_name), exist_ok=True)\n",
    "                sum += sp\n",
    "                list2.append(sp)\n",
    "                # l3\n",
    "                l3 = 0\n",
    "                for intermediate_node1 in G.neighbors(i_orig):\n",
    "                    for intermediate_node2 in G.neighbors(intermediate_node1):\n",
    "                        if G.has_edge(j_orig, intermediate_node2):\n",
    "                            l3 += 1 / math.sqrt(\n",
    "                                G.degree[intermediate_node1] * G.degree[intermediate_node2])\n",
    "                sum += l3\n",
    "                list2.append(l3)\n",
    "                # local path\n",
    "                lp = common_lp[i][j]\n",
    "                sum += lp\n",
    "                list2.append(lp)\n",
    "                #print(\"before der for edge\")\n",
    "                der = partition_to_lp(i_orig, j_orig, G, current_comm_list_der)\n",
    "                if der >= 0:\n",
    "                    sum += der\n",
    "                    list2.append(der)\n",
    "                else:\n",
    "                    print(\"problem non negative der\")\n",
    "                    sys.exit()\n",
    "                #print(\"before surp for edge\")\n",
    "                surp = partition_to_lp(i_orig, j_orig, G, current_comm_list_surp)\n",
    "                if surp >= 0:\n",
    "                    sum += surp\n",
    "                    list2.append(surp)\n",
    "                else:\n",
    "                    print(\"problem non negative surp\")\n",
    "                    sys.exit()\n",
    "                #print(\"before sbm for edge\")\n",
    "                sbm = partition_to_lp(i_orig, j_orig, G, current_comm_list_sbm)\n",
    "                if sbm >= 0:\n",
    "                    sum += sbm\n",
    "                    list2.append(sbm)\n",
    "                else:\n",
    "                    print(\"problem non negative sbm\")\n",
    "                    sys.exit()\n",
    "                eigen = partition_to_lp(i_orig, j_orig, G, current_comm_list_eigen)\n",
    "                if eigen >= 0:\n",
    "                    sum += eigen\n",
    "                    list2.append(eigen)\n",
    "                else:\n",
    "                    print(\"problem non negative eigen\")\n",
    "                    sys.exit()\n",
    "                # print(\"success\"+str(identity))\n",
    "            else:\n",
    "                # print(\"out of bound\")\n",
    "                for i in range(f_no):\n",
    "                    list2.append(0)\n",
    "\n",
    "            if t1 == m - 2:\n",
    "                if t_graph[m - 1].has_edge(e[0], e[1]):\n",
    "                    list2.append(1)\n",
    "                else:\n",
    "                    list2.append(0)\n",
    "\n",
    "            if edge_key in edges_dict:\n",
    "                # print(\"old value = \"+str(edges_dict[edge_key]))\n",
    "                temp_list = list(edges_dict[edge_key])\n",
    "                temp_list = temp_list + list2\n",
    "                edges_dict[edge_key] = temp_list\n",
    "                # print(\"new value = \" + str(edges_dict[edge_key]))\n",
    "            else:\n",
    "                edges_dict[edge_key] = list2\n",
    "\n",
    "        endtime = time.time()\n",
    "        currentDT = datetime.datetime.now()\n",
    "        print(str(currentDT))\n",
    "        file_all = open('./result_comm/current_all.txt', 'a')\n",
    "        text_final = str(identity) + \" slice = \" + str(t1) + \" nodes = \" + \\\n",
    "                     str(len(G.nodes)) + \" edges = \" + str(len(G.edges)) + \\\n",
    "                     \" time - \" + str(currentDT) + \"\\n\"\n",
    "        file_all.write(text_final)\n",
    "        print(text_final)\n",
    "        file_all.close()\n",
    "\n",
    "    for e in teCt:\n",
    "        edge_key = str(e[0]) + \"+\" + str(e[1])\n",
    "        list1.append(list(edges_dict[edge_key]))\n",
    "    f = np.array(list1)\n",
    "    np.take(f, np.random.permutation(f.shape[0]), axis=0, out=f)\n",
    "    # print(\"list1 = \" + str(list1))\n",
    "    print(\"after get feature\" + str(identity))\n",
    "    print(\"shape of feature = \" + str(f.shape))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = features_comm_opti(5,\"mit\")\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
